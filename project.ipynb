{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnKmhTwWyEnlxHPm04Fiq6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameera326/GenerativeAIB40/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5uHA5XEJcCC",
        "outputId": "219116b7-d9b2-4922-e44d-e6c41b0b2070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 7986.9189 - mae: 86.5650 - val_loss: 10716.3105 - val_mae: 97.2004\n",
            "Epoch 2/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8127.9248 - mae: 87.3345 - val_loss: 10150.8096 - val_mae: 94.2840\n",
            "Epoch 3/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7704.1685 - mae: 84.7520 - val_loss: 8560.2061 - val_mae: 85.5647\n",
            "Epoch 4/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5488.0771 - mae: 70.4824 - val_loss: 5181.6929 - val_mae: 64.3618\n",
            "Epoch 5/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2705.8740 - mae: 46.2163 - val_loss: 1674.2742 - val_mae: 31.0219\n",
            "Epoch 6/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 745.1612 - mae: 20.6099 - val_loss: 1342.8779 - val_mae: 26.7443\n",
            "Epoch 7/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 779.3398 - mae: 20.8893 - val_loss: 1437.8234 - val_mae: 28.2940\n",
            "Epoch 8/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 778.8994 - mae: 20.9786 - val_loss: 1341.3181 - val_mae: 26.6722\n",
            "Epoch 9/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 680.2219 - mae: 20.0796 - val_loss: 1351.2095 - val_mae: 26.7785\n",
            "Epoch 10/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 554.6321 - mae: 18.1208 - val_loss: 1306.8931 - val_mae: 25.9199\n",
            "Epoch 11/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 488.6196 - mae: 17.0924 - val_loss: 1313.4590 - val_mae: 25.8760\n",
            "Epoch 12/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 581.7841 - mae: 17.3969 - val_loss: 1289.5253 - val_mae: 25.3391\n",
            "Epoch 13/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 462.4692 - mae: 16.3011 - val_loss: 1259.9429 - val_mae: 24.7556\n",
            "Epoch 14/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 517.3712 - mae: 16.6293 - val_loss: 1238.2566 - val_mae: 24.2141\n",
            "Epoch 15/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 487.3295 - mae: 16.0827 - val_loss: 1223.5043 - val_mae: 23.7535\n",
            "Epoch 16/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 404.6195 - mae: 15.2044 - val_loss: 1188.3534 - val_mae: 22.9065\n",
            "Epoch 17/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 438.7350 - mae: 15.8110 - val_loss: 1182.3640 - val_mae: 22.5612\n",
            "Epoch 18/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 352.2099 - mae: 14.2978 - val_loss: 1177.3949 - val_mae: 22.1343\n",
            "Epoch 19/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 359.5938 - mae: 13.9389 - val_loss: 1147.6378 - val_mae: 21.2256\n",
            "Epoch 20/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 418.0691 - mae: 15.0463 - val_loss: 1121.7688 - val_mae: 20.4329\n",
            "Epoch 21/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 320.3795 - mae: 13.0091 - val_loss: 1118.5994 - val_mae: 20.1693\n",
            "Epoch 22/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 351.6024 - mae: 13.5561 - val_loss: 1133.9867 - val_mae: 20.1265\n",
            "Epoch 23/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 284.6949 - mae: 12.6666 - val_loss: 1068.6445 - val_mae: 19.0388\n",
            "Epoch 24/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 299.0664 - mae: 12.7167 - val_loss: 1148.7617 - val_mae: 20.2332\n",
            "Epoch 25/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 401.9310 - mae: 14.0383 - val_loss: 1099.6692 - val_mae: 19.1851\n",
            "Epoch 26/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 290.2729 - mae: 12.6638 - val_loss: 1043.9524 - val_mae: 18.4148\n",
            "Epoch 27/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 302.4175 - mae: 12.3090 - val_loss: 1048.7219 - val_mae: 18.4218\n",
            "Epoch 28/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 335.3633 - mae: 12.9722 - val_loss: 1034.9481 - val_mae: 18.1791\n",
            "Epoch 29/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 294.4284 - mae: 12.5271 - val_loss: 1045.0948 - val_mae: 18.2780\n",
            "Epoch 30/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 348.2225 - mae: 12.9837 - val_loss: 1063.4375 - val_mae: 18.5295\n",
            "Epoch 31/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 337.5101 - mae: 12.9963 - val_loss: 1008.4163 - val_mae: 17.9720\n",
            "Epoch 32/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 281.2173 - mae: 11.9519 - val_loss: 1011.7331 - val_mae: 17.9435\n",
            "Epoch 33/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 318.5455 - mae: 12.6561 - val_loss: 1003.5689 - val_mae: 17.8909\n",
            "Epoch 34/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 348.3751 - mae: 12.2482 - val_loss: 998.1850 - val_mae: 17.9192\n",
            "Epoch 35/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 417.7957 - mae: 14.0293 - val_loss: 980.8038 - val_mae: 17.8008\n",
            "Epoch 36/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 297.3009 - mae: 12.3556 - val_loss: 996.9492 - val_mae: 18.1210\n",
            "Epoch 37/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 314.3012 - mae: 12.0474 - val_loss: 987.8162 - val_mae: 18.0852\n",
            "Epoch 38/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 279.7983 - mae: 12.2307 - val_loss: 942.2779 - val_mae: 17.6464\n",
            "Epoch 39/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 362.2038 - mae: 12.8509 - val_loss: 963.3255 - val_mae: 17.8170\n",
            "Epoch 40/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 280.0353 - mae: 11.7125 - val_loss: 960.2501 - val_mae: 17.8376\n",
            "Epoch 41/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 330.8342 - mae: 12.5678 - val_loss: 963.5743 - val_mae: 17.9217\n",
            "Epoch 42/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 258.5573 - mae: 11.9677 - val_loss: 932.7524 - val_mae: 17.6590\n",
            "Epoch 43/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 299.7777 - mae: 12.7403 - val_loss: 955.3637 - val_mae: 17.9348\n",
            "Epoch 44/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 262.9519 - mae: 11.5844 - val_loss: 955.0571 - val_mae: 17.9463\n",
            "Epoch 45/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 265.8249 - mae: 11.4163 - val_loss: 919.4948 - val_mae: 17.5529\n",
            "Epoch 46/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 336.7719 - mae: 12.3216 - val_loss: 930.7843 - val_mae: 17.7806\n",
            "Epoch 47/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 235.8620 - mae: 11.2341 - val_loss: 932.0894 - val_mae: 17.8141\n",
            "Epoch 48/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 322.5180 - mae: 12.3023 - val_loss: 933.5063 - val_mae: 17.8732\n",
            "Epoch 49/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 323.4896 - mae: 12.2855 - val_loss: 942.9275 - val_mae: 17.9146\n",
            "Epoch 50/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 304.7083 - mae: 11.8029 - val_loss: 915.0243 - val_mae: 17.6627\n",
            "Epoch 51/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 263.2212 - mae: 10.9728 - val_loss: 914.6664 - val_mae: 17.6790\n",
            "Epoch 52/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 206.3706 - mae: 10.8578 - val_loss: 901.9354 - val_mae: 17.4910\n",
            "Epoch 53/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 249.6250 - mae: 11.2367 - val_loss: 915.5712 - val_mae: 17.7347\n",
            "Epoch 54/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 247.8923 - mae: 11.1470 - val_loss: 912.3513 - val_mae: 17.6480\n",
            "Epoch 55/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 268.6098 - mae: 11.6983 - val_loss: 885.4415 - val_mae: 17.2882\n",
            "Epoch 56/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 315.8075 - mae: 12.5304 - val_loss: 888.3403 - val_mae: 17.3888\n",
            "Epoch 57/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 249.5362 - mae: 11.5160 - val_loss: 912.1115 - val_mae: 17.5687\n",
            "Epoch 58/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 233.7805 - mae: 11.0734 - val_loss: 877.3029 - val_mae: 17.3377\n",
            "Epoch 59/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 251.8102 - mae: 11.5357 - val_loss: 886.1128 - val_mae: 17.3577\n",
            "Epoch 60/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 247.5430 - mae: 11.0170 - val_loss: 875.9265 - val_mae: 17.3199\n",
            "Epoch 61/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 254.3209 - mae: 11.4391 - val_loss: 922.9431 - val_mae: 17.6583\n",
            "Epoch 62/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 288.7921 - mae: 12.5536 - val_loss: 902.9083 - val_mae: 17.4162\n",
            "Epoch 63/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 287.9487 - mae: 11.7100 - val_loss: 873.0730 - val_mae: 17.2592\n",
            "Epoch 64/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 294.2073 - mae: 12.3101 - val_loss: 898.2781 - val_mae: 17.3412\n",
            "Epoch 65/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 265.8748 - mae: 11.8435 - val_loss: 903.1140 - val_mae: 17.3808\n",
            "Epoch 66/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 220.2611 - mae: 10.6186 - val_loss: 885.5334 - val_mae: 17.1552\n",
            "Epoch 67/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 310.4874 - mae: 11.9933 - val_loss: 899.2323 - val_mae: 17.3509\n",
            "Epoch 68/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 231.2164 - mae: 11.2422 - val_loss: 935.9888 - val_mae: 17.7588\n",
            "Epoch 69/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 275.1794 - mae: 12.1253 - val_loss: 904.2182 - val_mae: 17.3043\n",
            "Epoch 70/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 270.1501 - mae: 11.6727 - val_loss: 885.9192 - val_mae: 17.0770\n",
            "Epoch 71/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 264.5991 - mae: 12.1203 - val_loss: 900.6061 - val_mae: 17.2007\n",
            "Epoch 72/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 287.2942 - mae: 11.3608 - val_loss: 905.0400 - val_mae: 17.1987\n",
            "Epoch 73/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 231.8133 - mae: 10.7832 - val_loss: 865.8833 - val_mae: 17.1663\n",
            "Epoch 74/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 281.5495 - mae: 11.8940 - val_loss: 885.2394 - val_mae: 16.9966\n",
            "Epoch 75/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 276.4335 - mae: 11.4319 - val_loss: 885.7101 - val_mae: 17.0036\n",
            "Epoch 76/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 284.8532 - mae: 11.8543 - val_loss: 876.8035 - val_mae: 16.9114\n",
            "Epoch 77/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 292.2384 - mae: 11.0109 - val_loss: 868.8088 - val_mae: 16.9236\n",
            "Epoch 78/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 232.6374 - mae: 10.8949 - val_loss: 882.9337 - val_mae: 16.9309\n",
            "Epoch 79/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 263.1918 - mae: 11.6466 - val_loss: 862.6620 - val_mae: 16.9453\n",
            "Epoch 80/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 262.0027 - mae: 11.1229 - val_loss: 927.5188 - val_mae: 17.4709\n",
            "Epoch 81/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 230.7404 - mae: 11.1511 - val_loss: 865.0350 - val_mae: 16.8389\n",
            "Epoch 82/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 235.9847 - mae: 11.0468 - val_loss: 912.8802 - val_mae: 17.1592\n",
            "Epoch 83/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 241.9417 - mae: 10.6079 - val_loss: 888.1359 - val_mae: 16.8467\n",
            "Epoch 84/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 243.7379 - mae: 10.6676 - val_loss: 862.1655 - val_mae: 16.8414\n",
            "Epoch 85/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 208.6219 - mae: 10.3198 - val_loss: 865.3480 - val_mae: 16.7477\n",
            "Epoch 86/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 246.6656 - mae: 10.9536 - val_loss: 879.0956 - val_mae: 16.7504\n",
            "Epoch 87/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 218.2653 - mae: 10.9147 - val_loss: 877.8438 - val_mae: 16.6780\n",
            "Epoch 88/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 288.1053 - mae: 11.2115 - val_loss: 874.0793 - val_mae: 16.6389\n",
            "Epoch 89/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 274.6975 - mae: 11.3603 - val_loss: 874.3365 - val_mae: 16.6687\n",
            "Epoch 90/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 209.1659 - mae: 10.4521 - val_loss: 866.4227 - val_mae: 16.6137\n",
            "Epoch 91/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 238.2281 - mae: 10.8250 - val_loss: 882.5448 - val_mae: 16.7132\n",
            "Epoch 92/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 220.6066 - mae: 10.8008 - val_loss: 884.7894 - val_mae: 16.7378\n",
            "Epoch 93/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 240.9803 - mae: 10.6386 - val_loss: 859.8465 - val_mae: 16.6937\n",
            "Epoch 94/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 254.7580 - mae: 11.1748 - val_loss: 870.2462 - val_mae: 16.5513\n",
            "Epoch 95/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 267.8886 - mae: 11.1495 - val_loss: 880.0956 - val_mae: 16.6647\n",
            "Epoch 96/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 237.5714 - mae: 11.3168 - val_loss: 859.0243 - val_mae: 16.6150\n",
            "Epoch 97/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 240.0576 - mae: 11.2213 - val_loss: 852.4628 - val_mae: 16.8546\n",
            "Epoch 98/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 225.2603 - mae: 10.6139 - val_loss: 852.3412 - val_mae: 16.8140\n",
            "Epoch 99/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 221.6950 - mae: 10.5136 - val_loss: 950.5028 - val_mae: 17.8729\n",
            "Epoch 100/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 234.5224 - mae: 10.9900 - val_loss: 882.9949 - val_mae: 16.5280\n",
            "Epoch 101/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 257.5334 - mae: 10.8670 - val_loss: 878.5339 - val_mae: 16.5456\n",
            "Epoch 102/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 243.6498 - mae: 10.8487 - val_loss: 853.9235 - val_mae: 16.6712\n",
            "Epoch 103/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 243.9436 - mae: 11.5109 - val_loss: 855.8444 - val_mae: 16.6064\n",
            "Epoch 104/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 262.9406 - mae: 11.2555 - val_loss: 865.9998 - val_mae: 16.3981\n",
            "Epoch 105/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 230.6294 - mae: 10.7448 - val_loss: 868.3276 - val_mae: 16.4908\n",
            "Epoch 106/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 236.5596 - mae: 10.3664 - val_loss: 855.4109 - val_mae: 16.6127\n",
            "Epoch 107/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 230.5276 - mae: 10.7863 - val_loss: 865.9298 - val_mae: 16.4070\n",
            "Epoch 108/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 252.2924 - mae: 10.6941 - val_loss: 899.5020 - val_mae: 16.7182\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "RMSE: 19.49\n",
            "RÂ² Score: 0.2896\n",
            "Accuracy (Â±10 units): 55.68%\n",
            "Saved artifact at '/tmp/tmp0vze_y9d'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 18), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135641387105936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135641388233360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135643069478672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135643069477520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135643069479440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135643069480592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135643069474448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135643069480016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel('Data_Solar Radiation.xlsx', header=1)\n",
        "\n",
        "# Rename last column to Solar_Radiation\n",
        "df.columns = df.columns[:-1].tolist() + ['Solar_Radiation']\n",
        "\n",
        "# Convert numeric columns\n",
        "for col in df.columns:\n",
        "    if col not in ['From Date', 'To Date']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Drop missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=['From Date', 'To Date', 'Solar_Radiation'])\n",
        "y = df['Solar_Radiation']\n",
        "\n",
        "# Normalize\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ANN model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train with early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=16,\n",
        "          validation_split=0.1, callbacks=[early_stop], verbose=1)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "accuracy_within_10 = np.mean(np.abs(y_test - y_pred) <= 10) * 100\n",
        "\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"RÂ² Score: {r2:.4f}\")\n",
        "print(f\"Accuracy (Â±10 units): {accuracy_within_10:.2f}%\")\n",
        "\n",
        "# Save as TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open(\"solar_radiation_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel('Data_Solar Radiation.xlsx', header=1)\n",
        "df.columns = df.columns[:-1].tolist() + ['Solar_Radiation']\n",
        "\n",
        "# Clean and convert\n",
        "for col in df.columns:\n",
        "    if col not in ['From Date', 'To Date']:\n",
        "        # Attempt to convert to numeric, errors to NaN\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        # If conversion fails, try to convert to datetime\n",
        "        # But only if the column is still an object type after the first conversion:\n",
        "        if df[col].dtype == 'object':\n",
        "            try:\n",
        "                df[col] = pd.to_datetime(df[col])\n",
        "                # Extract relevant features from datetime\n",
        "                df[col + '_hour'] = df[col].dt.hour\n",
        "                df[col + '_day'] = df[col].dt.day\n",
        "                df[col + '_month'] = df[col].dt.month\n",
        "                df = df.drop(columns=[col])  # Drop original datetime column\n",
        "            except ValueError:\n",
        "                pass  # If datetime conversion fails, skip\n",
        "\n",
        "# Drop rows with NaNs *after* all conversions\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Explicitly select only numeric features for correlation\n",
        "numeric_features = df.select_dtypes(include=np.number).columns.tolist()\n",
        "# Remove 'Solar_Radiation' as it's the target\n",
        "numeric_features.remove('Solar_Radiation')\n",
        "\n",
        "# Now you can calculate correlations on numeric features only\n",
        "correlation = df[numeric_features].corrwith(df['Solar_Radiation']).abs().sort_values(ascending=False)\n",
        "top_features = correlation[0:17].index.tolist()  # top 17 features\n",
        "\n",
        "\n",
        "# ... (Rest of your code) ...\n",
        "\n",
        "# Feature matrix and target\n",
        "X = df[top_features]\n",
        "y = df['Solar_Radiation']\n",
        "\n",
        "# Standardize (works better with ReLU)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Deep ANN Model\n",
        "model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train with early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "model.fit(X_train, y_train, epochs=1000, batch_size=8,\n",
        "          validation_split=0.15, callbacks=[early_stop], verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "custom_accuracy = np.mean(np.abs(y_test - y_pred) / y_test <= 0.10) * 100\n",
        "\n",
        "# Display Results\n",
        "print(f\"\\nâœ… Final Model Evaluation:\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"RÂ² Score: {r2:.4f}\")\n",
        "print(f\"Custom Accuracy (Â±10%): {custom_accuracy:.2f}%\")\n",
        "\n",
        "# Save as TFLite model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open(\"solar_radiation_high_accuracy.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"\\nğŸ’¾ Model saved as 'solar_radiation_high_accuracy.tflite'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hzhvIrLLTkX",
        "outputId": "9f1820be-ac7e-445e-cf05-04cd138ee79d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 6728.2612 - mae: 76.7560 - val_loss: 1824.7640 - val_mae: 29.6444\n",
            "Epoch 2/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1352.7152 - mae: 25.4205 - val_loss: 1270.9331 - val_mae: 29.2291\n",
            "Epoch 3/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 573.8896 - mae: 19.1825 - val_loss: 940.3633 - val_mae: 22.1245\n",
            "Epoch 4/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 394.9984 - mae: 15.2745 - val_loss: 863.0643 - val_mae: 19.7692\n",
            "Epoch 5/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 449.6913 - mae: 16.3733 - val_loss: 875.9691 - val_mae: 21.8664\n",
            "Epoch 6/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 402.2251 - mae: 15.5814 - val_loss: 810.5679 - val_mae: 18.9064\n",
            "Epoch 7/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 370.5264 - mae: 15.7356 - val_loss: 794.8168 - val_mae: 18.8838\n",
            "Epoch 8/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 346.4727 - mae: 14.4537 - val_loss: 773.4407 - val_mae: 18.0917\n",
            "Epoch 9/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 320.0727 - mae: 14.0028 - val_loss: 790.2920 - val_mae: 18.2469\n",
            "Epoch 10/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 372.7366 - mae: 14.6298 - val_loss: 814.1295 - val_mae: 18.9880\n",
            "Epoch 11/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 361.7639 - mae: 15.1941 - val_loss: 756.7539 - val_mae: 17.6688\n",
            "Epoch 12/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 338.8712 - mae: 14.4164 - val_loss: 801.3131 - val_mae: 18.5578\n",
            "Epoch 13/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 286.3859 - mae: 13.2198 - val_loss: 756.4753 - val_mae: 17.4453\n",
            "Epoch 14/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 285.2194 - mae: 12.7789 - val_loss: 926.6788 - val_mae: 22.0315\n",
            "Epoch 15/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 290.8229 - mae: 13.0403 - val_loss: 767.5409 - val_mae: 17.7633\n",
            "Epoch 16/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 313.8882 - mae: 13.5502 - val_loss: 728.8311 - val_mae: 17.5219\n",
            "Epoch 17/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 255.2209 - mae: 12.7603 - val_loss: 748.0909 - val_mae: 18.0262\n",
            "Epoch 18/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 279.0443 - mae: 12.8915 - val_loss: 747.9511 - val_mae: 17.0781\n",
            "Epoch 19/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 313.9716 - mae: 14.2107 - val_loss: 770.2117 - val_mae: 18.2182\n",
            "Epoch 20/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 292.9962 - mae: 13.3486 - val_loss: 751.9822 - val_mae: 16.9118\n",
            "Epoch 21/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 247.8252 - mae: 11.8751 - val_loss: 789.2571 - val_mae: 17.8308\n",
            "Epoch 22/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 295.6836 - mae: 13.1984 - val_loss: 781.1601 - val_mae: 17.7955\n",
            "Epoch 23/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 232.4350 - mae: 12.0004 - val_loss: 732.2767 - val_mae: 17.4636\n",
            "Epoch 24/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 273.3998 - mae: 13.0127 - val_loss: 702.5270 - val_mae: 16.3696\n",
            "Epoch 25/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 186.4482 - mae: 10.6398 - val_loss: 713.2106 - val_mae: 17.5302\n",
            "Epoch 26/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 257.9557 - mae: 11.9789 - val_loss: 699.3663 - val_mae: 15.8254\n",
            "Epoch 27/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 208.9701 - mae: 11.5925 - val_loss: 737.7185 - val_mae: 16.8004\n",
            "Epoch 28/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 216.9574 - mae: 11.9522 - val_loss: 960.1306 - val_mae: 20.2177\n",
            "Epoch 29/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 248.9181 - mae: 12.6496 - val_loss: 746.5722 - val_mae: 16.3183\n",
            "Epoch 30/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 233.3415 - mae: 11.9123 - val_loss: 744.9385 - val_mae: 16.3644\n",
            "Epoch 31/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 217.6480 - mae: 11.6573 - val_loss: 743.4884 - val_mae: 17.4316\n",
            "Epoch 32/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 264.6594 - mae: 12.7907 - val_loss: 886.3658 - val_mae: 18.4226\n",
            "Epoch 33/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 233.6656 - mae: 12.3513 - val_loss: 771.5006 - val_mae: 17.4224\n",
            "Epoch 34/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 227.6257 - mae: 11.7701 - val_loss: 746.2187 - val_mae: 16.1429\n",
            "Epoch 35/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 212.3334 - mae: 11.1900 - val_loss: 866.9669 - val_mae: 18.2407\n",
            "Epoch 36/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 228.1202 - mae: 11.6464 - val_loss: 741.2751 - val_mae: 16.5776\n",
            "Epoch 37/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 241.7835 - mae: 12.2411 - val_loss: 832.2582 - val_mae: 17.1221\n",
            "Epoch 38/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 191.8370 - mae: 10.8693 - val_loss: 800.5832 - val_mae: 16.5258\n",
            "Epoch 39/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 215.9033 - mae: 11.5619 - val_loss: 756.2160 - val_mae: 16.1859\n",
            "Epoch 40/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 251.1521 - mae: 12.3077 - val_loss: 759.8405 - val_mae: 15.9154\n",
            "Epoch 41/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 238.0758 - mae: 12.0607 - val_loss: 734.5419 - val_mae: 16.9062\n",
            "Epoch 42/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 198.9861 - mae: 10.6865 - val_loss: 729.3436 - val_mae: 16.1761\n",
            "Epoch 43/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 182.3093 - mae: 10.3230 - val_loss: 733.7159 - val_mae: 16.6128\n",
            "Epoch 44/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 196.3667 - mae: 10.6378 - val_loss: 683.5280 - val_mae: 16.1582\n",
            "Epoch 45/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 188.9823 - mae: 10.7528 - val_loss: 663.4312 - val_mae: 15.8726\n",
            "Epoch 46/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 209.0540 - mae: 11.0194 - val_loss: 767.0013 - val_mae: 17.0923\n",
            "Epoch 47/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 222.6431 - mae: 11.3741 - val_loss: 819.9834 - val_mae: 18.3994\n",
            "Epoch 48/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 216.6815 - mae: 12.0449 - val_loss: 804.6040 - val_mae: 17.7520\n",
            "Epoch 49/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 214.5289 - mae: 11.7494 - val_loss: 703.5396 - val_mae: 16.6225\n",
            "Epoch 50/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 210.3697 - mae: 11.2625 - val_loss: 702.8661 - val_mae: 15.8365\n",
            "Epoch 51/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 192.9952 - mae: 10.7927 - val_loss: 739.8613 - val_mae: 16.7034\n",
            "Epoch 52/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 173.5525 - mae: 10.2457 - val_loss: 730.2355 - val_mae: 15.7444\n",
            "Epoch 53/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 189.7986 - mae: 10.8219 - val_loss: 719.8634 - val_mae: 15.2174\n",
            "Epoch 54/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 205.7893 - mae: 11.7515 - val_loss: 749.4548 - val_mae: 15.7147\n",
            "Epoch 55/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 154.6072 - mae: 10.1945 - val_loss: 772.4482 - val_mae: 16.4807\n",
            "Epoch 56/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 152.0047 - mae: 10.0341 - val_loss: 699.5609 - val_mae: 16.0839\n",
            "Epoch 57/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 153.5401 - mae: 9.7575 - val_loss: 701.9968 - val_mae: 15.2358\n",
            "Epoch 58/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 136.5374 - mae: 8.9935 - val_loss: 726.4708 - val_mae: 16.8163\n",
            "Epoch 59/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 196.8891 - mae: 11.0285 - val_loss: 672.4006 - val_mae: 16.1908\n",
            "Epoch 60/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 221.9613 - mae: 11.5107 - val_loss: 711.7642 - val_mae: 15.8468\n",
            "Epoch 61/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 146.8976 - mae: 9.4906 - val_loss: 683.9703 - val_mae: 15.4467\n",
            "Epoch 62/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 163.8802 - mae: 10.1156 - val_loss: 675.9589 - val_mae: 15.0675\n",
            "Epoch 63/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 152.9413 - mae: 9.3988 - val_loss: 670.2950 - val_mae: 15.7971\n",
            "Epoch 64/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 177.1855 - mae: 10.4187 - val_loss: 711.4335 - val_mae: 16.9622\n",
            "Epoch 65/1000\n",
            "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 146.8689 - mae: 9.4006 - val_loss: 737.8676 - val_mae: 15.2560\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\n",
            "âœ… Final Model Evaluation:\n",
            "RMSE: 22.70\n",
            "RÂ² Score: 0.0367\n",
            "Custom Accuracy (Â±10%): 54.55%\n",
            "Saved artifact at '/tmp/tmpceuoni3i'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 17), dtype=tf.float32, name='keras_tensor_19')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135641222601744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135641222595024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135641222597520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135641222599248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135641222610768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135641222598672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135641222600400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135641222595408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135641222597904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135641222603856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "\n",
            "ğŸ’¾ Model saved as 'solar_radiation_high_accuracy.tflite'\n"
          ]
        }
      ]
    }
  ]
}